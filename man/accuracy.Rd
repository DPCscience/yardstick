% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classes.R
\name{accuracy}
\alias{accuracy}
\alias{accuracy.data.frame}
\alias{accuracy.table}
\alias{accuracy.matrix}
\alias{accuracy_vec}
\alias{kap}
\alias{kap.data.frame}
\alias{kap.table}
\alias{kap.matrix}
\alias{kap_vec}
\title{Classification Metrics on Predited Classes}
\usage{
accuracy(data, ...)

\method{accuracy}{data.frame}(data, truth, estimate, na.rm = TRUE, ...)

\method{accuracy}{table}(data, ...)

\method{accuracy}{matrix}(data, ...)

accuracy_vec(truth, estimate, na.rm = TRUE, ...)

kap(data, ...)

\method{kap}{data.frame}(data, truth, estimate, na.rm = TRUE, ...)

\method{kap}{table}(data, ...)

\method{kap}{matrix}(data, ...)

kap_vec(truth, estimate, na.rm = TRUE, ...)
}
\arguments{
\item{data}{Either a \code{data.frame} containing the \code{truth} and \code{estimate}
columns, or a \code{table}/\code{matrix} where the true class results should be
in the columns of the table.}

\item{...}{Not currently used.}

\item{truth}{The column identifier for the true class results
(that is a \code{factor}). This should be an unquoted column name although
this argument is passed by expression and supports
\link[rlang:quasiquotation]{quasiquotation} (you can unquote column
names). For \code{_vec()} functions, a \code{factor} vector.}

\item{estimate}{The column identifier for the predicted class
results (that is also \code{factor}). As with \code{truth} this can be
specified different ways but the primary method is to use an
unquoted variable name. For \code{_vec()} functions, a \code{factor} vector.}

\item{na.rm}{A \code{logical} value indicating whether \code{NA}
values should be stripped before the computation proceeds.}
}
\value{
For \code{_vec()} functions, a single \code{numeric} value (or \code{NA}).
Otherwise, a \code{tibble} with columns \code{.metric} and \code{.estimate} and 1 row of
values. For grouped data frames, the number of rows returned will be the
same as the number of groups.
}
\description{
Accuracy is the proportion of the data that are
predicted correctly. Kappa is a similar measure but is normalized by
the accuracy that would be expected by chance alone and is very useful
when one or more classes have large frequency distributions.
}
\examples{
library(dplyr)
data("hpc_cv")

# The observed and predicted classes from a single
# assessment set (i.e. fold)
fold_1 <- hpc_cv \%>\%
  filter(Resample == "Fold01")

fold_1 \%>\% conf_mat(truth = obs, estimate = pred)

fold_1 \%>\% accuracy(truth = obs, estimate = pred)

fold_1 \%>\% kap(truth = obs, estimate = pred)

fold_1 \%>\% metrics(truth = obs, estimate = pred)

# Groups are respected so you can calculate
# metrics across multiple resamples at once
hpc_cv \%>\%
  group_by(Resample) \%>\%
  accuracy(obs, pred)

}
\references{
Cohen, J. (1960). "A coefficient of agreement for nominal
scales". \emph{Educational and Psychological Measurement}. 20 (1): 37â€“46.
}
\seealso{
\code{\link[=conf_mat]{conf_mat()}}, \code{\link[=metrics]{metrics()}}
}
\author{
Max Kuhn
}
\keyword{manip}
